\section{Symplectic model-redcution with a weighted inner product} \label{sec:normmor}

In this section we will combine the concept of model reduction with a weighted inner product in \cref{sec:mor.1} with the symplectic model reduction discussed in \cref{sec:mor.2}. We will discuss how the new method can be viewed as a natural extension to the original symplectic method. Finally we generalize the greedy method for the symplectic basis generation, and the symplectic model reduction of nonlinear terms to be compatible with any weighted inner product.

\subsection{Generalization of the symplectic projection} \label{sec:normmor.1}
As discussed in \cref{sec:mor.1}, proper error analysis of methods for solving partial differential equations often require using a weighted inner product. This is particularly important when dealing with Hamiltonian systems, where the system energy can induce a norm that is fundamental to the dynamics of the system.

Consider a Hamiltonian system of the form (\ref{eq:mor.8}) together with the weighted inner product defined in (\ref{eq:mor.3}) with $m=2n$. Also suppose that the solution $z$ to (\ref{eq:mor.8}) lies on a $2k$ dimensional symplectic subspace with the basis $A$. We would like to construct a projection operator that minimizes the projection error with respect to the $X$-norm while preserving the symplectic dynamics of (\ref{eq:mor.8}) in the projected space. Consider the operator $P: \mathbb R^{2n} \to \mathbb R^{2n}$ defined as
\begin{equation} \label{eq:normmor.1}
	P = A \mathbb J_{2k}^T A^T X \mathbb J_{2n} X.
\end{equation}
It is easy to show that $P$ is idempotent if and only if
\begin{equation} \label{eq:normmor.2}
	\mathbb J_{2k}^T A^T X \mathbb J_{2n} X A = I_{2k}.
\end{equation}
This means that $P$ is a projection operator onto the span space of $A$. Suppose that $S$ is the snapshot matrix containing the time samples $\{z(t_i)\}_{i=1}^N$ of the solution to (\ref{eq:mor.8}). We like to find the basis $A$ that minimizes the projection error of the samples in $S$ with respect to $P$.
\begin{equation} \label{eq:normmor.3}
\begin{aligned}
& \underset{A\in \mathbb{R}^{2n\times 2k}}{\text{minimize}}
& & \| S - P(S) \|_X, \\
& \text{subject to}
& & \mathbb J_{2k}^T A^T X \mathbb J_{2n} X A = I_{2k}.
\end{aligned}
\end{equation}
by (\ref{eq:normmor.1}) we have
\begin{equation} \label{eq:normmor.4}
\begin{aligned}
	\| S - P(S) \|_X &= \| S - A \mathbb J_{2k}^T A^T X \mathbb J_{2n} X S \|_X \\
	&= \| X^{1/2} S - X^{1/2} A \mathbb J_{2k}^T A^T X \mathbb J_{2n} X S \|_2 \\
	&= \| \tilde S - \tilde A \tilde A ^+ \tilde S \|_2.
\end{aligned}
\end{equation}
Here $\tilde S = X^{1/2} S$, $\tilde A = X^{1/2} A$ and $\tilde A^+ = \mathbb J_{2k}^T \tilde A^T J_{2n}$ is the sympletic inverse of $\tilde A$ with respect to the skew-symmetric matrix $J_{2n} = X^{1/2} \mathbb J_{2n} X^{1/2}$. Note that the symplectic inverse in (\ref{eq:normmor.4}) is a generalization of the symplectic inverse introduced in \cref{sec:mor.2}. Therefore, we may use the same notation (the superscript $+$) for both. We summarized the properties of this generalization in \Cref{thm:2}. With the introduced notations, the condition (\ref{eq:normmor.2}) turns into $\tilde A ^+ \tilde A = I$ which is equivalent to $\tilde A ^T J_{2n} \tilde A = \mathbb J_{2k}$. In other words, this condition implies that $\tilde A$ has to be a $J_{2n}$-symplectic matrix. Finally we can rewrite the minimization (\ref{eq:normmor.3}) as
\begin{equation} \label{eq:normmor.5}
\begin{aligned}
& \underset{A\in \mathbb{R}^{2n\times 2k}}{\text{minimize}}
& & \| \tilde S - P^\text{symp}_{X,\tilde A}(\tilde S) \|_2, \\
& \text{subject to}
& & \tilde A^T J_{2n} \tilde A = \mathbb J_{2k}.
\end{aligned}
\end{equation}
where $P^\text{symp}_{X,\tilde A} = \tilde A \tilde A^+$ is the symplectic projection with respect to the $X$-norm onto the span of $\tilde A$. At the first glance, the minimization (\ref{eq:normmor.5}) might look similar to (\ref{eq:mor.13}). However, since $\tilde A$ is $J_{2n}$-symplectic, and the projection operator depends on $X$, we need to seek for alternative approaches to find a near optimal solution to (\ref{eq:normmor.5}). 

Similar to (\ref{eq:mor.13}), direct approaches to solving (\ref{eq:normmor.5}) are impractical. Furthermore, there are no SVD-type methods known to the authors, that solves (\ref{eq:normmor.5}). However, the greedy generation of the symplectic basis can be generalized to generate a near optimal basis $\tilde A$. The generalized greedy method is discussed in \cref{sec:normmor.2}.

Now suppose that a basis $A$ that solves (\ref{eq:normmor.5}) is in hand such that $z = Ay$ with $y\in \mathbb R^{2k}$, the expansion coefficients of $z$ in the basis of $A$. Using (\ref{eq:normmor.2}) we may write the reduced system to (\ref{eq:mor.8}) as
\begin{equation} \label{eq:normmor.6}
	\dot y = \mathbb J_{2k}^T A^T X \mathbb J_{2n} X \mathbb{J}_{2n} LAy + \mathbb J_{2k}^T A^T X \mathbb J_{2n} X \mathbb{J}_{2n} \nabla_z f(z).
\end{equation}
Since $\nabla_z H(z) = Lz + \nabla_z f(z)$, we may use the chain rule to write
\begin{equation} \label{eq:normmor.7}
	\nabla_z H(z) = ( \mathbb J_{2k}^T A^T X \mathbb J_{2n} X )^T \nabla_y H(Ay).
\end{equation}
Finally the reduced system (\ref{eq:normmor.6}) simplifies to
\begin{equation} \label{eq:normmor.8}
\left\{
\begin{aligned}
	\dot y(t) &= J_{2k} A^T L A y + J_{2k} \nabla_y f(Ay), \\
	y(0) &= \tilde A^+ X^{1/2} z_0.
\end{aligned}
\right.
\end{equation}
where $J_{2k}=\tilde A^+ J_{2n} (\tilde A^+)^T$ is a skew-symmetric matrix. The system (\ref{eq:normmor.8}) is a generalized Hamiltonian system with the Hamiltonian defined as $\tilde H(y) = y^TA^TLAy + f(Ay)$. Therefore, a Poisson integrator can preserve the symplectic symmetry associated with (\ref{eq:normmor.8}). 


We close this section by summarizing the properties of the sympelctic inverse in the form of the following theorem.
\begin{theorem} \label{thm:2}
Let $A\in \mathbb R^{2n\times 2k}$ be a $J_{2n}$-symplectic basis where $J_{2n}\in\mathbb R^{2n}$ is a full rank and skew-symmetric matrix. Furthermore, suppose that $A^{+} = \mathbb{J}_{2k}^T A^T J_{2n}$ is the symplectic psudo-inverse. Then the following holds:
\begin{enumerate}
\item $A^+A = I_{2k}$.
\item $(A^+)^T$ is $J_{2n}^{-1}$-symplectic.
\item $\left(\left(\left(A^+\right)^T\right)^+\right)^T = A$.
\item Let $J_{2n}=X^{1/2}\mathbb J_{2n} X^{1/2}$. Then $A$ is ortho-normal with respect to the $X$-norm, if and only if $(A^+)^T$ is ortho-normal with respect to the $X^{-1}$-norm.
\end{enumerate}
\end{theorem}
\begin{proof}
it is straight forward to show all statements using the definition of a symplectic basis.
\end{proof}

\subsection{Greedy generation of a $J_{2n}$-symplectic basis} \label{sec:normmor.2}
In this section we modify the greedy algorithm introduced in \cref{sec:mor.3} to construct a $J_{2n}$-symplectic basis. Ortho-normalization is an essential step in most greedy approaches to basis generation \cite{hesthaven2015certified,quarteroni2015reduced}. Here, we summarize a variation of the Gram-Schmidt orthogonalization process, known as the \emph{symplectic Gram-Schmidt} process.

Suppose that $\Omega$ is a symplectic form defined on $\mathbb R^{2n}$ such that $\Omega(x,y) = x^T J_{2n} y$, for all $x,y\in \mathbb R^{2n}$ and some full rank and skew-symmetric matrix $J_{2n} = X^{1/2} \mathbb J_{2n} X^{1/2}$. We would like to build a basis of size $2k+2$ in an iterative manner. We start with some initial vector, e.g. $e_1 = z_0$. It is known that a symplectic basis is even dimensional \cite{Marsden:2010:IMS:1965128}. We may take $Te_1$, where $T = X^{-1/2} \mathbb J_{2n}^{T}X^{1/2}$ as a candidate for the second basis vector. It is easily checked that $\tilde A_2=[e_1|Te_1]$ is $J_{2n}$-symplectic and consequently, $\tilde A_2$ is the first basis generated by the greedy approach. Next, suppose that $\tilde A_{2k} = [e_1|\dots|e_k|Te_1|\dots|Te_k]$ is generated in $k$-th step of the greedy method and $z\not \in \text{span}\left(\tilde A_{2k}\right)$ is provided. We aim to $J_{2n}$-orthogonalize $z$ with respect to the basis $\tilde A_{2k}$. This means we should find coefficients $\alpha_i,\beta_i\in \mathbb R$, for $i=1,\dots,k$ such that
\begin{equation} \label{eq:normmor.9}
	\Omega\left( z +\sum_{i=1}^{k} \alpha_i e_i +\sum_{i=1}^{k} \beta_i Te_i, \sum_{i=1}^{k}\bar \alpha_i e_i +\sum_{i=1}^{k} \bar \beta_i Te_i \right) = 0,
\end{equation}
for all possible $\bar \alpha_i,\bar \beta_i \in \mathbb R$, $i=1,\dots,k$. It is easily checked that this problem has the unique solution
\begin{equation} \label{eq:normmor.10}
	\alpha_i = - \Omega(z,Te_i), \quad \beta = \Omega(z,e_i).
\end{equation}
If we take $\tilde z = z -\sum_{i=1}^{k} \Omega(z,Te_i) e_i +\sum_{i=1}^{k} \Omega(z,e_i) Te_i$, then the next candidate pair of basis vectors are $e_{k+1} = \tilde z / \| \tilde z \|_X$ and $Te_{k+1}$. Finally, the basis generated at the $(k+1)$th step of the greedy method is given by
\begin{equation} \label{eq:normmor.11}
	\tilde A_{2k+2} = [e_1|\dots|e_k|e_{k+1}|Te_1|\dots|Te_k|Te_{k+1}].
\end{equation}
It is checked easily that $\tilde A_{2k+2}$ is $J_{2n}$-symplectic. We point out that the symplectic Gram-Schmidt orthogonalization process is chosen due to its simplicity. However, in problems where there is a need for a large basis, this process might be impractical. In such cases, one may use a backward stable routine, e.g. the isotropic Arnoldi method or the isotropic Lanczos method \cite{doi:10.1137/S1064827500366434}.

It is well known that symplectic bases, in general, are not norm bounded \cite{doi:10.1137/050628519}. The following theorem guarantees that the greedy method for generating a $J_{2n}$-symplectic basis yields a bounded basis.
\begin{theorem}
The basis generated by the greedy method for constructing a $J_{2n}$-symplectic basis is ortho-normal with respect to the $X$-norm.
\end{theorem}
\begin{proof}
Let $\tilde A_{2k}=[e_1|\dots,e_k|Te_1|\dots|Te_k]$ be the $J_{2n}$-symplectic basis generated at the $k$th step of the greedy method. Using the fact that $\tilde A_{2k}$ is $J_{2n}$-symplectic, one can check that
\begin{equation} \label{eq:normmor.12}
	[e_i,e_j]_X = [Te_i,Te_j]_X = \Omega(e_i,Te_j)=\delta_{i,j}, \quad i,j=1,\dots,k,	
\end{equation}
and
\begin{equation} \label{eq:normmor.13}
	[e_i,Te_j]_X = \Omega(e_i,e_j) = 0\quad i,j=1,\dots,k,
\end{equation}
where $\delta_{i,j}$ is the Kronecker delta function. This shows that $\tilde A_{2k}^TX\tilde A_{2k} = I_{2k}$, i.e., $\tilde A_{2k}$ is an ortho-normal basis with respect to the $X$-norm.
\end{proof}
We point out that if we take $X=I_{2n}$, then the greedy process generates a $\mathbb J_{2n}$- symplectic basis. As the matter of fact, with this choice, the greedy method discussed above becomes identical to the greedy process discussed in \cref{sec:mor.3}, since $T = X^{-1/2}\mathbb J_{2n}^TX^{1/2} = \mathbb J_{2n}^T$.

For identifying the best vectors to be added to a set of basis vectors, we may use similar error functions to those introduced in \cref{sec:mor.3}. The projection error can be used to identify the snapshot that is worst approximated by a given basis $\tilde A_{2k}$:
\begin{equation} \label{eq:normmor.14}
	t^{k+1} := \underset{t}{\text{argmax } }\| X^{1/2}z(t) - P^\text{symp}_{X,\tilde A_{2k}}(X^{1/2}z(t)) \|_2. 
\end{equation}
Or alternatively we can use the loss in the Hamiltonian function in (\ref{eq:mor.16}) for parameter dependent problems. We summarized the greedy method for generating a $J_{2n}$-symplectic matrix in \Cref{alg:2}.

\begin{algorithm} 
\caption{The greedy algorithm for generation of a $J_{2n}$-symplectic basis} \label{alg:2}
{\bf Input:} Tolerated projection error $\delta$, initial condition $ z_0$, the snapshots $\{\tilde z(t_i)\}_{i=1}^{N} = \{X^{1/2} z(t_i)\}_{i=1}^{N}$, full rank matrix $X=X^T>0$
\begin{enumerate}
\item $T \leftarrow X^{-1/2}\mathbb J_{2n}^T X^{1/2}$
\item $t^1 \leftarrow t=0$
\item $e_1 \leftarrow X^{1/2}z_0$
\item $\tilde A \leftarrow [e_1|Te_1]$
\item $k \leftarrow 1$
\item \textbf{while} $\| \tilde z(t) - P^\text{symp}_{X,\tilde A}( \tilde z(t) ) \|_2 > \delta$ for all $t \in [0,T]$
\item \hspace{0.5cm} $t^{k+1} := \underset{t\in [0,T]}{\text{argmax }} \| \tilde z(t) - P^\text{symp}_{X,\tilde A}( \tilde z(t) ) \|_2$
\item \hspace{0.5cm} $J_{2n}$-orthogonalize $ \tilde z(t^{k+1})$ to obtain $e_{k+1}$
\item \hspace{0.5cm} $\tilde A \leftarrow [e_1|\dots |e_{k+1} | Te_1|\dots| Te_{k+1}]$
\item \hspace{0.5cm} $k \leftarrow k+1$
\item \textbf{end while}
\item $A\leftarrow X^{-1/2} \tilde A$
\end{enumerate}
\vspace{0.5cm}
{\bf Output:} $J_{2n}$-symplectic basis $\tilde A$ and the reduced basis $A$
\end{algorithm}

It is shown in \cite{doi:10.1137/17M1111991} that under natural assumptions on the solution manifold of (\ref{eq:mor.8}), the original greedy method for symplectic basis generation converges exponentially fast. We expect similar convergence rate for the generalized greedy method, since the $X$-norm is topologically equivalent to the standard Euclidean norm \cite{friedman1970foundations}, for a full rank matrix $X$.

\subsection{Efficient evaluation of nonlinear terms} \label{sec:normmor.3}
Evaluating the nonlinear term in (\ref{eq:normmor.8}) still contain a computational complexity proportional to the size of the full order system (\ref{eq:mor.8}). To overcome this inefficiency, we may take a similar approach to \cref{sec:mor.2}. The DEIM approximation of the nonlinear term in (\ref{eq:normmor.8}) yields
\begin{equation} \label{eq:normmor.15}
	\dot y = J_{2k} A^TLAy + \tilde A ^+ X^{1/2} \mathbb J_{2n} U (\mathcal P^TU)^{-1}\mathcal  P^T \nabla_z f(z).
\end{equation}
Here $U$ is a basis for the nonlinear snapshots $\{\nabla_z f(z(t_i))\}_{i=1}^N$, and $\mathcal P$ is the interpolating index matrix \cite{Chaturantabut:2010cz}. As discussed in \cref{sec:mor.2}, for a general choice of $U$, the reduced system (\ref{eq:normmor.8}) does not take a Hamiltonian form. Applying the chain rule on (\ref{eq:normmor.15}) gives
\begin{equation} \label{eq:normmor.16}
	\dot y = J_{2k} A^TLAy + \tilde A ^+ X^{1/2} \mathbb J_{2n} U (\mathcal P^TU)^{-1} \mathcal P^T X^{1/2} (\tilde A^+)^T \nabla_y f(Ay).
\end{equation}
Now if we require $U = X^{1/2} (\tilde A^+)^T$ then the complex expression in (\ref{eq:normmor.16}) reduces to
\begin{equation} \label{eq:normmor.17}
	\dot y = J_{2k} A^TLAy + J_{2k} \nabla_y f(Ay),
\end{equation}
and hence we recover the Hamiltonian structure. This gives rise to the reduced system
\begin{equation} \label{eq:normmor.18}
\left\{
\begin{aligned}
	\dot y(t) &= J_{2k} A^TLAy + J_{2k} (\mathcal P^TX^{1/2} (\tilde A^+)^T)^{-1} \mathcal P^T \nabla_z f(z), \\
	y(0) &= \tilde A^+ X^{1/2} z_0.
\end{aligned}
\right.
\end{equation}

We now discuss how to ensure $X^{1/2} (\tilde A^+)^T$ to be a basis for the nonlinear snapshots. Note that if $z \in \text{span}\left(X^{1/2} (\tilde A^+)^T\right)$ then $X^{-1/2} z \in \text{span}\left(( \tilde A^+)^T \right)$. Therefore, it is sufficient to require $(\tilde A^+)^T$ to be a basis for $\{X^{-1/2} \nabla_z f(z(t_i))\}_{i=1}^N$. \Cref{thm:2} indicates that $(\tilde A^+)^T$ is a $J_{2n}^{-1}$-symplectic basis and that the transformation between $\tilde A$ and $(\tilde A^+)^T $ does not affect the symplectic feature of the bases. Consequently, from $A$ we may compute $(\tilde A^+)^T$ and enrich it with snapshots $\{X^{-1/2} \nabla_z f(z(t_i))\}_{i=1}^N$. Once $(\tilde A^+)^T$ represents the nonlinear term with the desired accuracy, we may compute $\tilde A= \left( \left( ( \tilde A^+ )^T \right)^+ \right)^T$ to obtain the reduced basis for (\ref{eq:normmor.18}). Note that \Cref{thm:2} implies that $(\tilde A^+)^T$ is ortho-normal with respect to the $X^{-1}$-norm. This affects the ortho-normalization process. We summarized the process of generating a basis for the nonlinear terms in \Cref{alg:3}.

\begin{algorithm} 
\caption{Generation of a basis for nonlinear terms} \label{alg:3}
{\bf Input:} Tolerated projection error $\delta$, $J_{2n}$-symplectic basis $\tilde A$ of size $2k$, the snapshots $\{\tilde z(t_i)\}_{i=1}^{N} = \{X^{-1/2} \nabla_zf(z(t_i))\}_{i=1}^{N}$, full rank matrix $X=X^T>0$
\begin{enumerate}
\item $T \leftarrow X^{1/2}\mathbb J_{2n}^T X^{-1/2}$
\item compute $(\tilde A^+)^T$
\item \textbf{while} $\| \tilde z(t) - P^\text{symp}_{X^{-1},(\tilde A^+)^T}( \tilde z(t) ) \|_2 > \delta$ for all $t \in [0,T]$
\item \hspace{0.5cm} $t^{k+1} := \underset{t\in [0,T]}{\text{argmax }} \| \tilde z(t) - P^\text{symp}_{X^{-1},(\tilde A^+)^T}( \tilde z(t) ) \|_2$
\item \hspace{0.5cm} $J_{2n}^{-1}$-orthogonalize $ \tilde z(t^{k+1})$ to obtain $e_{k+1}$
\item \hspace{0.5cm} $(\tilde A^+)^T \leftarrow [e_1|\dots |e_{k+1} | Te_1|\dots| Te_{k+1}]$
\item \hspace{0.5cm} $k \leftarrow k+1$
\item \textbf{end while}
\item $\tilde A \leftarrow \left( \left( ( \tilde A^+ )^T \right)^+ \right)^T$
\end{enumerate}
\vspace{0.5cm}
{\bf Output:} $J_{2n}$-symplectic basis $\tilde A$
\end{algorithm}

\subsection{Offline/online decomposition} \label{sec:normmor.4}
Model-reduction becomes particularly useful in multi-query settings. For the purpose of most efficient computation, it is important to delineate high dimensional ($\mathcal{O}(n)$) computations from low dimensional ($\mathcal{O}(k)$) ones in the implementation phase. Time intensive high dimensional quantities are computed only once for the given problem in offline phase and cheaper low dimensional computations can be performed in the online phase. This segregation or compartmentalization of quantities according to their computational cost is referred to as offline/online decomposition.

More precisely, one can decompose the computations into following stages:
\paragraph{Offline stage:} Quantities in this stage are computed only once for use in the next stage.
\begin{enumerate}
\item Generate weighted snapshots matrix $X^{1/2}S$ and basis $U$ of nonlinear snapshots $\{\nabla_zf(z(t_i))\}_{i=1}^N$ by solving the high dimensional system \cref{eq:mor.8}.
\item Generate greedy $J_{2n}$-symplectic basis and basis for nonlinearity using \Cref{alg:2,alg:3}, respectively.
\item Assemble the reduced order model \cref{eq:mor.12}.
\end{enumerate}

\paragraph{Online stage:} Reduced model \cref{eq:mor.12} is simulated for multiple parameter sets and desired output is saved in this stage.

We use this decomposition to perform experiments in the next section.